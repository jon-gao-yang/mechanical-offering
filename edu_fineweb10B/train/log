04-12-25, 6AM:
(venv) santiago@santiago:~/Desktop/mechanical-offering$ PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True python nGPT_train.py --batch_size=32 --compile=False --n_layer=4 --n_head=4 --n_embd=64 --block_size=256 --max_iters=1000 >> edu_fineweb10B/train/log

Current Directory: /home/santiago/Desktop/mechanical-offering
Overriding: batch_size = 32
Overriding: compile = False
Overriding: n_layer = 4
Overriding: n_head = 4
Overriding: n_embd = 64
Overriding: block_size = 256
Overriding: max_iters = 1000
tokens per iteration will be: 524,288
found 100 shards for split train
found 1 shards for split val
Initializing a new model from scratch
defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)
number of parameters: 6.70M
Model initialization/loading time: 0.061843 sec
num decayed parameter tensors: 26, with 6,701,056 parameters
num non-decayed parameter tensors: 17, with 53,120 parameters
using fused AdamW: False
learning_rate: 0.001500
min_lr: 0.000000
max_iters: 1000.000000
lr_decay_iters: 600000.000000
warmup_iters: 0.000000
batch_size: 32.000000
gradient_accumulation_steps: 64.000000
block_size: 256.000000
weight_decay: 0.000000
Time spent: 2.026759624481201 seconds
starting_iter_num: 0
step 0: train loss 10.833044, val loss 10.833035
iter 0: loss 10.830959, time 9649.37ms
lr=0.001500
iter 10: loss 10.758569, time 2801.68ms
iter 20: loss 10.646104, time 2798.78ms
iter 30: loss 10.494774, time 2803.97ms
iter 40: loss 10.302562, time 2797.88ms
iter 50: loss 10.147997, time 2800.25ms
iter 60: loss 9.895168, time 2797.61ms
iter 70: loss 9.629969, time 2805.20ms
iter 80: loss 9.426777, time 2804.85ms
iter 90: loss 9.237202, time 2802.33ms
iter 100: loss 9.024031, time 2803.81ms
lr=0.001500
iter 110: loss 8.830610, time 2803.38ms
iter 120: loss 8.801071, time 2809.43ms
iter 130: loss 8.647708, time 2799.99ms
iter 140: loss 8.534380, time 2805.55ms
iter 150: loss 8.399112, time 2804.80ms
iter 160: loss 8.160581, time 2804.51ms
iter 170: loss 8.190888, time 2809.91ms
iter 180: loss 8.086080, time 2801.75ms
iter 190: loss 8.008282, time 2800.71ms
iter 200: loss 7.910591, time 2806.71ms
lr=0.001500
iter 210: loss 7.803560, time 2805.36ms
iter 220: loss 7.645970, time 2800.95ms
iter 230: loss 7.710441, time 2809.82ms
iter 240: loss 7.589214, time 2802.98ms
iter 250: loss 7.395622, time 2807.46ms
iter 260: loss 7.434982, time 2799.59ms
iter 270: loss 7.335974, time 2804.90ms
iter 280: loss 7.360983, time 2807.87ms
iter 290: loss 7.580942, time 2800.76ms
iter 300: loss 7.205116, time 2806.93ms
lr=0.001500
iter 310: loss 7.129072, time 2801.51ms
iter 320: loss 7.197424, time 2807.42ms
iter 330: loss 7.099652, time 2805.89ms
iter 340: loss 7.125823, time 2806.82ms
iter 350: loss 7.197876, time 2809.18ms
iter 360: loss 7.251622, time 2805.02ms
iter 370: loss 7.062611, time 2812.17ms
iter 380: loss 6.703791, time 2807.36ms
iter 390: loss 6.892871, time 2809.46ms
iter 400: loss 6.653235, time 2807.09ms
lr=0.001500
iter 410: loss 6.828910, time 2810.43ms
iter 420: loss 6.704347, time 2808.21ms
iter 430: loss 6.556508, time 2805.96ms
iter 440: loss 6.630173, time 2808.19ms
iter 450: loss 6.534611, time 2804.47ms
iter 460: loss 6.553273, time 2810.91ms
iter 470: loss 6.508822, time 2814.04ms
iter 480: loss 6.682697, time 2802.52ms
iter 490: loss 6.365928, time 2821.74ms
iter 500: loss 6.408033, time 2813.71ms
lr=0.001500
iter 510: loss 7.133717, time 2823.32ms
iter 520: loss 6.424999, time 2825.57ms
iter 530: loss 6.416022, time 2818.28ms
iter 540: loss 6.332326, time 2822.33ms
iter 550: loss 6.352127, time 2822.05ms
iter 560: loss 6.269125, time 5415.20ms
iter 570: loss 6.715741, time 2815.40ms
iter 580: loss 6.177294, time 2820.67ms
iter 590: loss 6.620299, time 2818.12ms
iter 600: loss 6.300308, time 2819.05ms
lr=0.001500
iter 610: loss 6.141992, time 2824.07ms
iter 620: loss 6.220259, time 2816.53ms
iter 630: loss 6.033041, time 2821.92ms
iter 640: loss 6.192245, time 2819.56ms
iter 650: loss 6.127841, time 2818.95ms
iter 660: loss 6.380894, time 2822.88ms
iter 670: loss 6.276946, time 2816.60ms
iter 680: loss 6.065897, time 2820.62ms
iter 690: loss 6.014085, time 2820.97ms
iter 700: loss 6.143736, time 2816.58ms
lr=0.001500
iter 710: loss 6.020860, time 2822.80ms
iter 720: loss 5.880308, time 2818.18ms
iter 730: loss 6.079087, time 2823.30ms
iter 740: loss 6.677000, time 2823.30ms
iter 750: loss 6.105751, time 2825.21ms
iter 760: loss 6.208768, time 2824.21ms
iter 770: loss 6.111234, time 2821.09ms
iter 780: loss 6.162528, time 2821.12ms
iter 790: loss 5.586231, time 2820.98ms
iter 800: loss 5.920054, time 2823.86ms
lr=0.001500
iter 810: loss 5.758483, time 2824.22ms
iter 820: loss 5.917303, time 2820.03ms
iter 830: loss 5.928635, time 2824.98ms
iter 840: loss 5.708065, time 2820.08ms
iter 850: loss 5.835686, time 2818.61ms
iter 860: loss 5.538291, time 2819.59ms
iter 870: loss 6.052765, time 2816.41ms
iter 880: loss 5.840627, time 2823.67ms
iter 890: loss 6.014676, time 2825.42ms
iter 900: loss 5.795800, time 2814.93ms
lr=0.001500
iter 910: loss 5.948944, time 2824.61ms
iter 920: loss 5.725521, time 2817.01ms
iter 930: loss 5.915332, time 2819.64ms
iter 940: loss 5.823522, time 2817.34ms
iter 950: loss 5.831390, time 2823.33ms
iter 960: loss 5.708886, time 2820.80ms
iter 970: loss 6.030440, time 2818.59ms
iter 980: loss 5.771375, time 2821.70ms
iter 990: loss 5.561573, time 2817.20ms
step 1000: train loss 5.783288, val loss 5.767238
saving checkpoint to ./edu_fineweb10B/train
Checkpoint saving time: 0.081299 sec
iter 1000: loss 5.940297, time 9271.80ms
lr=0.001500

SAMPLING:
<|endoftext|>For others as well, we’t be ‘t know that the problem’ and they do to a day’s personal research,, is a common factor for a new year with information. “A/H-1-B. are good.”
How to ask the study of the children of the work of the government’s relationship is the second one of the people about whether and the word is not a common role by any and about one with its important understanding of the world. The project of the first year with a particular event, and then of a new study is the best of the data.
The latest “c/I’t understand I am.” She has a lot of the project – to be sure’s. Because one will be able to get the difference. For example, the fact, the study was already a result of the findings. He was only not for their development of the
You
Time spent: 2848.4870870113373 seconds
